# Database and ETL Pipeline for Sparkify

## Objective
> Postgred database and ETL pipeline designed to optimize queries on song play analysis for Sparkify

## Table of contents
* [Technologies](#technologies)
* [Launch](#launch)
* [Database Schema](#database-schema)
* [Datasets](#datasets)
* [Files in Repository](#files-in-repository)
* [IMPORTANT NOTES](#important-notes)

## Technologies
Project is created with:
- Python 3
- Postgresql

## Launch
- Run code line in `start.ipynb`

## Database Schema

#### Fact Table:
        
##### songplays - [records in log data associated with song plays]
- songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
                    
#### Dimension Tables:

##### users - [users in the app]
- user_id, first_name, last_name, gender, level
            
##### songs - [songs in music database]
- song_id, title, artist_id, year, duration
            
##### artists - [artists in music database]
- artist_id, name, location, latitude, longitude (ie. [Artists_table](/readme_images/artists_table.png))

##### time - [timestamps of records in songplays broken down into specific units]
- start_time, hour, day, week, month, year, weekday (ie. [Time_table](/readme_images/time_table.png))

## Datasets

### Song Dataset
- Subset of data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/)
- JSON files contain metadata about songs and artists

### Song Dataset
- JSON log files generated by [eventsim](https://github.com/Interana/eventsim) based on above dataset

## Files/Folder in Repository

- `start.ipynb` - executes `create_tables.py` and `etl.py`

- `test.ipynb` - displays the first few rows of each table to let you check your database.

- `create_tables.py` - drops and creates tables. Run this file to reset your tables before each time you run your ETL scripts.

- `etl.ipynb` - reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.

- `etl.py` - reads and processes files from `song_data` and `log_data` and loads them into your tables. You can fill this out based on your work in the ETL notebook.

- `sql_queries.py` - contains all your sql queries, and is imported into the last three files above.

- **data** - contains all log's and song's data '.json' files

## IMPORTANT NOTES: 

> You will not be able to run `test.ipynb`, `etl.ipynb`, or `etl.py` until you have run `create_tables.py` at least once to create the `sparkifydb` database, which these other files connect to.

> Each time you run `test.ipynb`, close the connection to the database(`sparkifydb`). Multiple connections on same database not allowed.